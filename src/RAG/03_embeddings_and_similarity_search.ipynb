{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2894de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "data_dir = os.environ['DATA_DIR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac5ce9",
   "metadata": {},
   "source": [
    "# Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cc1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunit\\AppData\\Local\\Temp\\ipykernel_31692\\1927410493.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe773c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I like books\"\n",
    "sentence2 = \"I enjoy reading books\"\n",
    "sentence3 = \"It has been a hectic day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab501493",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = embeddings.embed_query(sentence1)\n",
    "embeddings2 = embeddings.embed_query(sentence2)\n",
    "embeddings3 = embeddings.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac2ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66e5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9589619330182793 0.9589619330182793\n",
      "0.7619478769782573 0.7619478769782573\n",
      "0.7729534751301653 0.7729534751301653\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(embeddings1, embeddings2), cosine_similarity(embeddings1, embeddings2))\n",
    "print(np.dot(embeddings1, embeddings3), cosine_similarity(embeddings1, embeddings3))\n",
    "print(np.dot(embeddings2, embeddings3), cosine_similarity(embeddings2, embeddings3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ce465",
   "metadata": {},
   "source": [
    "# Generate Embeddings for CS229 ML lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3cab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd36990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate data added on purpose to simulate messy data\n",
    "pdfs = [\n",
    "    \"machinelearning-lecture01.pdf\",\n",
    "    \"machinelearning-lecture01.pdf\",\n",
    "    \"MachineLearning-Lecture02.pdf\",\n",
    "    \"MachineLearning-Lecture03.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3447a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyPDFLoader(f\"{data_dir}\\\\RAG\\\\PDF\\\\{pdf}\")\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78f5825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3424196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a69c08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from pathlib import Path\n",
    "persist_directory = Path(f\"{data_dir}\\\\RAG\\\\VectorStore\\\\Qdrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ceb833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunit\\AppData\\Local\\Temp\\ipykernel_31692\\3537487967.py:5: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "C:\\Users\\gunit\\AppData\\Local\\Temp\\ipykernel_31692\\3537487967.py:14: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  vectorstore = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(\n",
    "    path=persist_directory,\n",
    "    prefer_grpc=False\n",
    ")\n",
    "client.recreate_collection(\n",
    "    collection_name=\"cs229_ml_lectures\",\n",
    "    vectors_config={\n",
    "        \"vector\": {\n",
    "            \"size\": 1536,\n",
    "            \"distance\": \"Cosine\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "vectorstore = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=\"cs229_ml_lectures\",\n",
    "    embeddings=embeddings,\n",
    "    vector_name=\"vector\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90783677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80cc80e6fd8e41318c4bfdf2e2bd5538',\n",
       " 'a844e1648dba4360935539299b96aebc',\n",
       " '9f986d396fb34eec820826b91f0d348e',\n",
       " '8e590b8726d94bca920b0b33167221e8',\n",
       " '3be956617cc6489881fadb5f1be42347',\n",
       " 'fdc44732c93845308febbeb1b749d68e',\n",
       " '482fe5b55e604580b63bfe52ff8f4a6a',\n",
       " '53dad13105074992b124de4feb7d16f7',\n",
       " '8ab3139f9c30481186343fdc21347e01',\n",
       " '78800096d71f43ba918fb2c86717d0e0',\n",
       " '82535f9e35ed4f6d925fac5103550449',\n",
       " 'e0ab6ee28cd14d49b35e282a32623517',\n",
       " '8c3b88f395a24cd99b37c6b49ededed5',\n",
       " '7bf4a521f1df45a5a9985d151f8fddf6',\n",
       " '821e8e2e5a23458bb3bf2490e8b3c8fa',\n",
       " 'dc3f215492554a399df77436df9966b6',\n",
       " 'af52642d98854c918833fcf3403c1689',\n",
       " '2ccb617b3241453881de677954852fbc',\n",
       " 'bbe2551abbc04ca38a428be73640bffa',\n",
       " '0d274271e52446e5a9162dc78fc96953',\n",
       " 'f66d31b8c63b45a2a91a5eb8ca81ae03',\n",
       " 'fcdf00d04c764e789e6e0693c28454ff',\n",
       " 'c766f6b41f13490d9f616d4c4d41d40c',\n",
       " '3c6dec6d467942beafe0e22c6836a8f3',\n",
       " '240aa5f72afc4035946b3aa120b87be1',\n",
       " 'd17eef392e6e4fe88991e7f0223d9e78',\n",
       " '22af7dcbe63b4e06bac9e11b566955d4',\n",
       " 'cddcacd0099c41f389afd14453e74cb7',\n",
       " '876bae6c7ad747b8b077fbbf97159217',\n",
       " 'ecab162e7dde4cae8f7146b9c490da0b',\n",
       " '242787768d12442bbdfdd441b220366d',\n",
       " '03a6ce822dca4b7e852f335a1e9b015b',\n",
       " 'aecb3b6f8443403484df49cc60c35e16',\n",
       " 'aa237ae07f064ea58da34d4c2bf34bc6',\n",
       " '72d4e72f50de43f3bc427a3610fd4929',\n",
       " '113fd5cd893a479b9581790204ff7d96',\n",
       " 'bf90db5b49f94ab69b5facabd184eda8',\n",
       " 'fc13f12077194dcf9c440aeb714768a6',\n",
       " 'd64db2e5bd4349c1b5a06c168659b3de',\n",
       " '4dca407982e24239a270e70245a028dc',\n",
       " '4a446b393ed3443ea5313e1b9d896e6d',\n",
       " 'fa4c28d7729e40698d3b1170f1933450',\n",
       " '4102339e602e488d8339fa60f58452e2',\n",
       " '60c9d7cf76514b7481e507ac343ce0a3',\n",
       " 'b15a58ee0dff464bac9713d23ec478b7',\n",
       " 'aec8b1de171146689046e0334c5b3484',\n",
       " 'fb296d1dee9f4820852426c08cbd2501',\n",
       " '4aab5e0aba7047a282019f8dac0266c4',\n",
       " '49c3438f95d441a5835dd6b3ba6c5cd8',\n",
       " '900720dc121445fe81cd5c30762b6a52',\n",
       " '048b9655c81f4cc1bdc309acd91288da',\n",
       " 'b67c0454b8564a9eb0c9c3d93e30d0a6',\n",
       " 'c281058cb00b4e9fbc5b176b17cbc9b4',\n",
       " '10fc20043a3f4659962392d3139cef15',\n",
       " '7cca80a377f14091b884118af565dc66',\n",
       " '6d5205bc68574311b31db62893706918',\n",
       " '6c99ea2d5b184592b96b73e77ef6762e',\n",
       " '08fbf753c5d74615bbc5efd9235c12df',\n",
       " 'd7db1dabd6834425965037d0e4a6863a',\n",
       " 'c1286057ddb843eb84c8fd90118d4416',\n",
       " '03a6ce296d794edda482ed68bcc3d4e6',\n",
       " 'dbcd3e8df45c47479e65490429d4fc87',\n",
       " 'e9f01547ceff4abdbf6c54c34db196a7',\n",
       " '62f8084af39f4bde9316025deb1737bd',\n",
       " 'da7b03fa30b34caa9194016859a77e63',\n",
       " '7a2146e118494deb91c72ac6a407ccbc',\n",
       " 'e447cd2f8e19413381e93f699307e094',\n",
       " '6692c57a653d4544aa58292c1fb2adef',\n",
       " 'ad58d320ce3540faa19d5203ba5d78de',\n",
       " 'bd3d767a6c0e4858a0370520d4f6e985',\n",
       " 'e12f39a6b5cb482eb78fb7742cb0afaa',\n",
       " 'e6e9612937244f588b8d65969b6e6451',\n",
       " 'e3ddf8cbba0746958d9683c90f89d789',\n",
       " '519841184f514e938889954b4b1a544d',\n",
       " '240ead27fb4f42078b4287cb14227c3a',\n",
       " '01f5f1ae4e334c7b9422d6ce6dd421a8',\n",
       " 'dd258e19f48f478c9f65f1e10efe2d56',\n",
       " '67bd2e978c6844c0afe34303d78f5623',\n",
       " 'be389ebc903d4a359a089d460399b197',\n",
       " '7f5534a37e304cbdb9af3df3384610e3',\n",
       " '624c8676f911439f8470a8c132bd04bb',\n",
       " '18c1dc34ad414ae5892050b5e7dea1a4',\n",
       " 'bbc25b9264bd4690914e657a73f04e1e',\n",
       " '97949f27d30d4fc097ec58d99244fd53',\n",
       " 'b5244dbfc1c74fa6bfb383ef963a7028',\n",
       " '78ebe52734c543e1a99ea86e04b995c1',\n",
       " 'aea1107b7e284f30b2c562d3bd9a39bf',\n",
       " 'd169353ecccf4939b2cf674156be57b4',\n",
       " '9362d237548346b890be1354c4ad8ce7',\n",
       " 'cc260bcd60cc4218baf31142e29a1cf0',\n",
       " '8b99b5a18f4846be8d355cb8d82901d0',\n",
       " '9f9b562414b240d588ef37f8a7a9fd5d',\n",
       " '84e89c8865124fbf810784525d1848db',\n",
       " '878023bf3fbd47309fe8c3abf1a26e5d',\n",
       " 'fab1aca8d47f457ba9e98c8fbbf6cb38',\n",
       " '8f2b3efb111144b28c15045231caccc0',\n",
       " 'dd18cd8a256a480d83d9df046585d226',\n",
       " 'b6b9d2c6d60f45ef8bd6633e04b2cf5e',\n",
       " 'd0325aa107324f9c8d27b80d46945e27',\n",
       " 'd8e87aeb425b472a8c17265567990045',\n",
       " '0ba1a0c20c484f12936a595bcca3bbf6',\n",
       " 'dee79b5ea4f748fea57164c1899060c6',\n",
       " '0be8269822c247f28a760c1a27d21444',\n",
       " '5994c77eb7ab4d2194a1de700ee6fd2c',\n",
       " '6dff52876bc248e29d1f992438ec9f21',\n",
       " '86c1d6d4ba1043eaa5af91a855152df2',\n",
       " '3a4ea0ef5a5148ef81cd56aa237e4f00',\n",
       " '48642550bb3b4bba981cab0e6388d35a',\n",
       " '548bc80982bb45d489eaac7bec6ea2c8',\n",
       " '802ad4681c874bbfa3ff7c89b7a0d9f2',\n",
       " '9b7fa7e5663646859b5433be80ae05ea',\n",
       " '7677ee64fdba47649becb0d517c1e149',\n",
       " 'af83c566cad94a0a99e4608991a68272',\n",
       " '0168e9e895e547c2b3ec1babcd43ffbf',\n",
       " 'a0fdc91eb5e64e93b0447fdde8415f17',\n",
       " '0abfa7461ef04d3c9d35ec1d7f5e050d',\n",
       " '422e50abcbb94a3e93127553ec50aa7a',\n",
       " 'c9924522575245019d8f0f251dffb6ee',\n",
       " '71ea23f1de9048af8ba15a974ae3a958',\n",
       " '62f5ed43ec2946b2a4b26d50f5435314',\n",
       " '20268ca827424aae883b16b666af8e5e',\n",
       " 'ef295d225abc4796b327443e3ca3aad2',\n",
       " 'a99283372a194f359c3403b1f4003f89',\n",
       " '2440cafa91a14af8a6de40c0d6cd295d',\n",
       " 'c64adceb4ee6423cbf36edaff2f0e39d',\n",
       " '886fba4187eb44ffbf46940db1ffc254',\n",
       " '09d3ffe453eb45e38c36fd9b598efc01',\n",
       " 'ef6142e1436f472f9ed8ce9bf7c68e69',\n",
       " '893cfd012cde4791bd415928043e1f61',\n",
       " '2a5d463b0750458da9d72f5a3000923f',\n",
       " 'beda5aa92655469e9eb772265e56cbc5',\n",
       " 'ee4b74bcecbe4b1d810d3b93694c4b73',\n",
       " 'c981f7d04f004125b556d5f4391db7f6',\n",
       " '7582e7d07b5b4c43a51f655e8989eed2',\n",
       " '16f3d2b1faf647a890dd46441991717a',\n",
       " 'dd7d2df0f4d14b2c85fd1495a747d3cf',\n",
       " '154057ba806045379a3bd01248352823',\n",
       " '86409a3db045435b802fde2efd311bb5',\n",
       " 'cb57e0f706f54d16814f1d9784ced304',\n",
       " 'e566e001da1e40a29ef9960338b80277',\n",
       " '5bb95c32f47948f19ed6bb5a0224e752',\n",
       " 'ed7de78e873e4b0c9684100fb12a0556',\n",
       " 'a517e390b202486c9022f8467af28dba',\n",
       " '74d7da1767424ed5b30ec8b800b3ae34',\n",
       " '3903d5ae21a4467096655cec5a3122d2',\n",
       " '192864f27adf4c1ea94297c777d824ef',\n",
       " 'b0fe16699c3d4785b43217a47e6de80b',\n",
       " '0778a47a20d04048a3349c2e16becbbb',\n",
       " '4df09ea945fd4c56960545d8d7d39d74',\n",
       " 'e274a121b2c04a5d8c2191da1f0137cb',\n",
       " '0c447612ea7247559c2892ef9acb6469',\n",
       " '8466ebec43974a539fdcb9db84bd3fe9',\n",
       " 'c3f317f2fd3144149f53cd46bebec66d',\n",
       " 'b3ec0cf2064448bf8cc3f4d919dc4147',\n",
       " 'd29d125db47a4c218ecbb4cb1f1f548d',\n",
       " '159691247777438895b9b7f661e35aee',\n",
       " 'bbd9fcdaa1864e7682f5e4e4bc3f4dbf',\n",
       " 'bf0cbd855b3a4ed78ef1884aec033340',\n",
       " 'd25e375f5b4b4857ada01acd8129fc37',\n",
       " 'bb9d75c44c3540d8a1c585cf9dbfe23a',\n",
       " '0819719316554461bb1d66e7e92af83d',\n",
       " '2cf15b9bd6614b309daf9ed95f28459d',\n",
       " 'bfa81ba1d3894bab90bc4f58fb7aa771',\n",
       " 'c4e556e74ac949ffba3f4ce2bc606d6d',\n",
       " '247b1432fc9d44d79df2bf48a58c5e72',\n",
       " '551a2ea4fa954c4ab827604b3ffaf60a',\n",
       " 'ab1bd873ca4d4223b13a9e1d7c20b176',\n",
       " 'ae1685937ae647658095cc0997659014',\n",
       " '7e88a61fe1c34f188269c0f9fe61a664',\n",
       " '4f05d78ae6a14e6789744d756550f4ae',\n",
       " 'e2d72f85717a43fc9c9147190aaaf7cd',\n",
       " '7372de33e4fe4826a8f4fa2d9954ee8a',\n",
       " '840a64e3f0284635a6cb0b259d574484',\n",
       " 'f88fc73d89174ba8a69cde9da0f4c8c6',\n",
       " 'a80a96908ae94582861d9720c2fea7d1',\n",
       " 'c30e052b92284dba9dd927e742557b5e',\n",
       " 'a062bd49466544a1a4f68c08d2f970c0',\n",
       " '722c84f34f134065b52f1c7c6852b87e',\n",
       " 'd7a13105fc244b42917790f4050f1ea2',\n",
       " '630cef1974d546e39beaf6ade784c7bd',\n",
       " '745ea15855014fecb8753d6024c8cf6d',\n",
       " 'a2fc2499271646faa024b2b519b68a0a',\n",
       " 'ee1a7eb61f62422f9f65dca94c5d7344',\n",
       " 'a4275a10f40c43c0a4da18153dce20f1',\n",
       " 'a57439eaaa6f449a92816f4db50c53e5',\n",
       " 'f1703b03e19a4cf685b9bef50e24b8d8',\n",
       " '7729b75361e64865ab73c10fb65371a3',\n",
       " '94f30615ee974e76bb1488e15b83d448',\n",
       " '538f18f432ef4b79a8379b6426944f52',\n",
       " '87a2cfce87974e5e8d33d58cb5db5d0f',\n",
       " 'c8589a2eeb404b7b8729b3ac45b41d11',\n",
       " '1e6358d6571d42d6af1288a120e6aae5',\n",
       " '24b9640cdd084416a7084c9be3cfc213',\n",
       " '4f77d2c369374d9f9cbf120bc4909de0',\n",
       " '29bc8321d8684329bbb04cfe0670ef5f',\n",
       " 'b9a5c56b752b4e86b7bd81fe7335aa47',\n",
       " '461075c4b6c0469d9b5dbed406d9bbe6',\n",
       " '0fe7502ea7dd4838a1e7048b5e887f22',\n",
       " 'a73c5e7cfa5049868ebc90372bb79ede',\n",
       " 'a23161cb65564a18a51f240bbac767ad',\n",
       " '0b22431f586a4f159a2e1372df1e330f',\n",
       " 'bbc30ead2e2d4276bacbb09c7770b425',\n",
       " 'c8c7c81d0fce4f44891e02f8ba3a41ff',\n",
       " '88bb46b30c324290a6526de298f38adf',\n",
       " '6bb89a46f0144727b9e539fdb932f8ab',\n",
       " '0fae8317c08941eaaf0011660af1908f',\n",
       " 'f71b85c8322f4123aed35c86e0728818',\n",
       " '5dac113e27444a8296672b0cac3e8fd4']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1190958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=208\n"
     ]
    }
   ],
   "source": [
    "print(client.count(collection_name=\"cs229_ml_lectures\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68350bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', '_id': '821e8e2e5a23458bb3bf2490e8b3c8fa', '_collection_name': 'cs229_ml_lectures'}, page_content=\"cs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework problems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thing that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this class more is if you form a study \\ngroup.  \\nSo start looking around where you're sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also post on the class newsgroup if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this class are reasonably difficult. People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a better learning experience if you form a\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', '_id': 'e6e9612937244f588b8d65969b6e6451', '_collection_name': 'cs229_ml_lectures'}, page_content=\"cs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework problems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thing that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this class more is if you form a study \\ngroup.  \\nSo start looking around where you're sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also post on the class newsgroup if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this class are reasonably difficult. People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a better learning experience if you form a\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', '_id': 'e12f39a6b5cb482eb78fb7742cb0afaa', '_collection_name': 'cs229_ml_lectures'}, page_content=\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, and I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the technical content of this class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in the class to get to know each other and \\nhave whatever discussions you want to have amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to talk trash about this class there.  \\nIf you want to contact the teaching staff, please use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is there an email I can ask for help?\"\n",
    "vectorstore.similarity_search(query=question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c0d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', '_id': 'be389ebc903d4a359a089d460399b197', '_collection_name': 'cs229_ml_lectures'}, page_content='those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people call it a free version of MATLAB, which it sort of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t seen MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to \\nwrite codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than \\nMATLAB, but it\\'s free, and for the purposes of this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine learning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, like, ten years ago came \\ninto his office and he said, \"Oh, professor, professor, thank you so much for your'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', '_id': 'fcdf00d04c764e789e6e0693c28454ff', '_collection_name': 'cs229_ml_lectures'}, page_content='those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people call it a free version of MATLAB, which it sort of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t seen MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to \\nwrite codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than \\nMATLAB, but it\\'s free, and for the purposes of this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine learning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, like, ten years ago came \\ninto his office and he said, \"Oh, professor, professor, thank you so much for your'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', '_id': 'c766f6b41f13490d9f616d4c4d41d40c', '_collection_name': 'cs229_ml_lectures'}, page_content='into his office and he said, \"Oh, professor, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s helped me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion \\nsections will be taught by the TAs, and attendance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televised. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or three weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', '_id': '7f5534a37e304cbdb9af3df3384610e3', '_collection_name': 'cs229_ml_lectures'}, page_content='into his office and he said, \"Oh, professor, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s helped me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion \\nsections will be taught by the TAs, and attendance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televised. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or three weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\machinelearning-lecture01.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', '_id': '240aa5f72afc4035946b3aa120b87be1', '_collection_name': 'cs229_ml_lectures'}, page_content=\"So later this quarter, we'll use the discussion sections to talk about things like convex \\noptimization, to talk a little bit about hidden Markov models, which is a type of machine \\nlearning algorithm for modeling time series and a few other things, so extensions to the \\nmaterials that I'll be covering in the main lectures. And attendance at the discussion \\nsections is optional, okay?  \\nSo that was all I had from logistics. Before we move on to start talking a bit about \\nmachine learning, let me check what questions you have. Yeah?  \\nStudent : [Inaudible] R or something like that?  \\nInstructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \\nwelcome to use R, but I would strongly advise against it, mainly because in the last \\nproblem set, we actually supply some code that will run in Octave but that would be \\nsomewhat painful for you to translate into R yourself. So for your other assignments, if \\nyou wanna submit a solution in R, that's fine. But I think MATLAB is actually totally \\nworth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \\nbit more often for various reasons. Yeah?  \\nStudent : For the [inaudible] project [inaudible]?  \\nInstructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \\ngroups of three, or you're welcome to do it by yourself or in groups of two. Grading is the \\nsame regardless of the group size, so with a larger group, you probably — I recommend\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did they say about matlab?\"\n",
    "vectorstore.similarity_search(query=question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90f84f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', '_id': '2cf15b9bd6614b309daf9ed95f28459d', '_collection_name': 'cs229_ml_lectures'}, page_content='MachineLearning-Lecture03  \\nInstructor (Andrew Ng):Okay. Good morning and welcome back to the third lecture of \\nthis class. So here’s what I want to do today, and some of the topics I do today may seem \\na little bit like I’m jumping, sort of, from topic to topic, but here’s, sort of, the outline for \\ntoday and the illogical flow of ideas. In the last lecture, we talked about linear regression \\nand today I want to talk about sort of an adaptation of that called locally weighted \\nregression. It’s very a popular algorithm that’s actually one of my former mentors \\nprobably favorite machine learning algorithm.  \\nWe’ll then talk about a probable second interpretation of linear regression and use that to \\nmove onto our first classification algorithm, which is logistic regression; take a brief \\ndigression to tell you about something called the perceptron algorithm, which is \\nsomething we’ll come back to, again, later this quarter; and time allowing I hope to get to \\nNewton’s method, which is an algorithm for fitting logistic regression models.  \\nSo this is recap where we’re talking about in the previous lecture, remember the notation \\nI defined was that I used this X superscript I, Y superscript I to denote the I training \\nexample. And when we’re talking about linear regression or linear least squares, we use \\nthis to denote the predicted value of “by my hypothesis H” on the input XI. And my \\nhypothesis was franchised by the vector of grams as theta and so we said that this was'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', '_id': 'c8c7c81d0fce4f44891e02f8ba3a41ff', '_collection_name': 'cs229_ml_lectures'}, page_content='Student:It’s the lowest it –  \\nInstructor (Andrew Ng):No, exactly. Right. So zero to the same, this is not the same, \\nright? And the reason is, in logistic regression this is different from before, right? The \\ndefinition of this H subscript theta of XI is not the same as the definition I was using in \\nthe previous lecture. And in particular this is no longer theta transpose XI. This is not a \\nlinear function anymore. This is a logistic function of theta transpose XI. Okay? So even \\nthough this looks cosmetically similar, even though this is similar on the surface, to the \\nBastrian descent rule I derived last time for least squares regression this is actually a \\ntotally different learning algorithm. Okay? And it turns out that there’s actually no \\ncoincidence that you ended up with the same learning rule. We’ll actually talk a bit more \\nabout this later when we talk about generalized linear models. But this is one of the most \\nelegant generalized learning models that we’ll see later. That even though we’re using a \\ndifferent model, you actually ended up with what looks like the same learning algorithm \\nand it’s actually no coincidence. Cool.  \\nOne last comment as part of a sort of learning process, over here I said I take the \\nderivatives and I ended up with this line. I didn’t want to make you sit through a long \\nalgebraic derivation, but later today or later this week, please, do go home and look at our'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', '_id': '630cef1974d546e39beaf6ade784c7bd', '_collection_name': 'cs229_ml_lectures'}, page_content='data sets as well. So don’t want to talk about that. If you’re interested, look up the work \\nof Andrew Moore on KD-trees. He, sort of, figured out ways to fit these models much \\nmore efficiently. That’s not something I want to go into today. Okay? Let me move one. \\nLet’s take more questions later.  \\nSo, okay. So that’s locally weighted regression. Remember the outline I had, I guess, at \\nthe beginning of this lecture. What I want to do now is talk about a probabilistic \\ninterpretation of linear regression, all right? And in particular of the – it’ll be this \\nprobabilistic interpretation that let’s us move on to talk about logistic regression, which \\nwill be our first classification algorithm. So let’s put aside locally weighted regression for \\nnow. We’ll just talk about ordinary unweighted linear regression. Let’s ask the question \\nof why least squares, right? Of all the things we could optimize how do we come up with \\nthis criteria for minimizing the square of the area between the predictions of the \\nhypotheses and the values Y predicted. So why not minimize the absolute value of the \\nareas or the areas to the power of four or something? What I’m going to do now is \\npresent one set of assumptions that will serve to “justify” why we’re minimizing the sum \\nof square zero. Okay?  \\nIt turns out that there are many assumptions that are sufficient to justify why we do least \\nsquares and this is just one of them. So just because I present one set of assumptions'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', '_id': 'ae1685937ae647658095cc0997659014', '_collection_name': 'cs229_ml_lectures'}, page_content='regression problem like this. What I want to do today is talk about a class of algorithms \\ncalled non-parametric learning algorithms that will help to alleviate the need somewhat \\nfor you to choose features very carefully. Okay? And this leads us into our discussion of \\nlocally weighted regression. And just to define the term, linear regression, as we’ve \\ndefined it so far, is an example of a parametric learning algorithm. Parametric learning \\nalgorithm is one that’s defined as an algorithm that has a fixed number of parameters that \\nfit to the data. Okay? So in linear regression we have a fix set of parameters theta, right? \\nThat must fit to the data. In contrast, what I’m gonna talk about now is our first non-\\nparametric learning algorithm. The formal definition, which is not very intuitive, so I’ve \\nreplaced it with a second, say, more intuitive. The, sort of, formal definition of the non-\\nparametric learning algorithm is that it’s an algorithm where the number of parameters \\ngoes with M, with the size of the training set. And usually it’s defined as a number of \\nparameters grows linearly with the size of the training set. This is the formal definition. A \\nslightly less formal definition is that the amount of stuff that your learning algorithm \\nneeds to keep around will grow linearly with the training sets or, in another way of saying \\nit, is that this is an algorithm that we’ll need to keep around an entire training set, even'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', '_id': 'bbc30ead2e2d4276bacbb09c7770b425', '_collection_name': 'cs229_ml_lectures'}, page_content='least squares regression being a bad idea for classification problems and then I did a \\nbunch of math and I skipped some steps, but I’m, sort of, claiming at the end they’re \\nreally the same learning algorithm?  \\nStudent:[Inaudible] constants?  \\nInstructor (Andrew Ng):Say that again.  \\nStudent:[Inaudible]  \\nInstructor (Andrew Ng):Oh, right. Okay, cool.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', '_id': '29bc8321d8684329bbb04cfe0670ef5f', '_collection_name': 'cs229_ml_lectures'}, page_content='predictions of your hypothesis have changed completely if your threshold – your \\nhypothesis at Y equal both 0.5. Okay? So –  \\nStudent:In between there might be an interval where it’s zero, right? For that far off \\npoint?  \\nInstructor (Andrew Ng):Oh, you mean, like that?  \\nStudent:Right.  \\nInstructor (Andrew Ng):Yeah, yeah, fine. Yeah, sure. A theta set like that so. So, I \\nguess, these just – yes, you’re right, but this is an example and this example works. This \\n–  \\nStudent:[Inaudible] that will change it even more if you gave it all –  \\nInstructor (Andrew Ng):Yeah. Then I think this actually would make it even worse. \\nYou would actually get a line that pulls out even further, right? So this is my example. I \\nget to make it whatever I want, right? But the point of this is that there’s not a deep \\nmeaning to this. The point of this is just that it could be a really bad idea to apply linear \\nregression to classification algorithm. Sometimes it work fine, but usually I wouldn’t do \\nit. So a couple of problems with this. One is that, well – so what do you want to do for \\nclassification? If you know the value of Y lies between zero and one then to kind of fix \\nthis problem let’s just start by changing the form of our hypothesis so that my hypothesis \\nalways lies in the unit interval between zero and one. Okay? So if I know Y is either zero \\nor one then let’s at least not have my hypothesis predict values much larger than one and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:05-07:00', 'author': '', 'moddate': '2008-07-11T11:25:05-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture02.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', '_id': '0abfa7461ef04d3c9d35ec1d7f5e050d', '_collection_name': 'cs229_ml_lectures'}, page_content=\"really makes a difference between a good solution and amazing solution. And to give \\neveryone to just how we do points assignments, or what is it that causes a solution to get \\nfull marks, or just how to write amazing solutions. Becoming a grader is usually a good \\nway to do that.  \\nGraders are paid positions and you also get free food, and it's usually fun for us to sort of \\nhang out for an evening and grade all the assignments. Okay, so I will send email. So \\ndon't email me yet if you want to be a grader. I'll send email to the entire class later with \\nthe administrative details and to solicit applications. So you can email us back then, to \\napply, if you'd be interested in being a grader.  \\nOkay, any questions about that? All right, okay, so let's get started with today's material. \\nSo welcome back to the second lecture. What I want to do today is talk about linear \\nregression, gradient descent, and the normal equations. And I should also say, lecture \\nnotes have been posted online and so if some of the math I go over today, I go over rather \\nquickly, if you want to see every equation written out and work through the details more \\nslowly yourself, go to the course homepage and download detailed lecture notes that \\npretty much describe all the mathematical, technical contents I'm going to go over today.  \\nToday, I'm also going to delve into a fair amount – some amount of linear algebra, and so\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:05-07:00', 'author': '', 'moddate': '2008-07-11T11:25:05-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture02.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', '_id': '20268ca827424aae883b16b666af8e5e', '_collection_name': 'cs229_ml_lectures'}, page_content=\"Instructor (Andrew Ng):All right, so who thought driving could be that dramatic, right? \\nSwitch back to the chalkboard, please. I should say, this work was done about 15 years \\nago and autonomous driving has come a long way. So many of you will have heard of the \\nDARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \\nteam's drive a car across a desert by itself.  \\nSo Alvin was, I think, absolutely amazing work for its time, but autonomous driving has \\nobviously come a long way since then. So what you just saw was an example, again, of \\nsupervised learning, and in particular it was an example of what they call the regression \\nproblem, because the vehicle is trying to predict a continuous value variables of a \\ncontinuous value steering directions, we call the regression problem.  \\nAnd what I want to do today is talk about our first supervised learning algorithm, and it \\nwill also be to a regression task. So for the running example that I'm going to use \\nthroughout today's lecture, you're going to return to the example of trying to predict \\nhousing prices. So here's actually a dataset collected by TA, Dan Ramage, on housing \\nprices in Portland, Oregon.  \\nSo here's a dataset of a number of houses of different sizes, and here are their asking \\nprices in thousands of dollars, $200,000. And so we can take this data and plot it, square \\nfeet, best price, and so you make your other dataset like that. And the question is, given a\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', '_id': 'a80a96908ae94582861d9720c2fea7d1', '_collection_name': 'cs229_ml_lectures'}, page_content='weighting function that falls of relatively slowly with distance from your query. Okay?  \\nSo I hope you can, therefore, see that if you apply locally weighted linear regression to a \\ndata set that looks like this, then to ask what your hypothesis output is at a point like this \\nyou end up having a straight line making that prediction. To ask what kind of class this \\n[inaudible] at that value you put a straight line there and you predict that value. It turns \\nout that every time you try to vary your hypothesis, every time you ask your learning \\nalgorithm to make a prediction for how much a new house costs or whatever, you need to \\nrun a new fitting procedure and then evaluate this line that you fit just at the position of \\nthe value of X. So the position of the query where you’re trying to make a prediction. \\nOkay? But if you do this for every point along the X-axis then you find that locally \\nweighted regression is able to trace on this, sort of, very non-linear curve for a data set \\nlike this. Okay?  \\nSo in the problem set we’re actually gonna let you play around more with this algorithm. \\nSo I won’t say too much more about it here. But to finally move on to the next topic let \\nme check the questions you have. Yeah?  \\nStudent:It seems like you still have the same problem of overfitting and underfitting, like \\nwhen you had a Q’s tow. Like you make it too small in your –  \\nInstructor (Andrew Ng):Yes, absolutely. Yes. So locally weighted regression can run'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00', 'author': '', 'moddate': '2008-07-11T11:25:03-07:00', 'title': '', 'source': 'C:\\\\\\\\Users\\\\\\\\gunit\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\Study Material\\\\\\\\Practice Projects\\\\\\\\remote\\\\\\\\artificial-intelligence\\\\\\\\data\\\\RAG\\\\PDF\\\\MachineLearning-Lecture03.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', '_id': 'bfa81ba1d3894bab90bc4f58fb7aa771', '_collection_name': 'cs229_ml_lectures'}, page_content='hypothesis was franchised by the vector of grams as theta and so we said that this was \\nequal to some from theta J, si J, and more theta transpose X. And we had the convention \\nthat X subscript Z is equal to one so this accounts for the intercept term in our linear \\nregression model. And lowercase n here was the notation I was using for the number of \\nfeatures in my training set. Okay? So in the example when trying to predict housing \\nprices, we had two features, the size of the house and the number of bedrooms. We had \\ntwo features and there was – little n was equal to two. So just to finish recapping the \\nprevious lecture, we defined this quadratic cos function J of theta equals one-half, \\nsomething I equals one to m, theta of XI minus YI squared where this is the sum over our \\nm training examples and my training set. So lowercase m was the notation I’ve been \\nusing to denote the number of training examples I have and the size of my training set. \\nAnd at the end of the last lecture, we derive the value of theta that minimizes this \\nenclosed form, which was X transpose X inverse X transpose Y. Okay?  \\nSo as we move on in today’s lecture, I’ll continue to use this notation and, again, I realize \\nthis is a fair amount of notation to all remember, so if partway through this lecture you \\nforgot – if you’re having trouble remembering what lowercase m is or what lowercase n \\nis or something please raise your hand and ask. When we talked about linear regression')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did they say about regression in the third lecture?\"\n",
    "vectorstore.similarity_search(query=question, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
